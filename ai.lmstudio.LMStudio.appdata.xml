<?xml version="1.0" encoding="UTF-8"?>
<component type="desktop">
  <id>ai.lmstudio.LMStudio</id>

  <name>LM Studio</name>
  <summary>Discover, download, and run local LLMs</summary>

  <developer_name>lmstudio.ai</developer_name>

  <metadata_license>CC0-1.0</metadata_license>
  <project_license>LicenseRef-proprietary</project_license>

  <supports>
    <control>pointing</control>
    <control>keyboard</control>
    <control>touch</control>
  </supports>

  <description>
    <p>
      LM Studio is an innovative application that empowers you to run large language models directly on your laptop, entirely offline. With LM Studio, you can interact with advanced AI models through an intuitive in-app chat interface or an OpenAI-compatible local server, providing flexible ways to integrate AI into your workflow. The software also allows you to download compatible model files from Hugging Face repositories, ensuring you always have access to the latest AI technology. Additionally, the Discover page offers a curated view of new and noteworthy LLMs, enabling you to explore a wide range of models and features that cater to your specific needs.
    </p>
  </description>

  <launchable type="desktop-id">ai.lmstudio.LMStudio.desktop</launchable>

	<screenshots>
		<screenshot type="default">
			<image>
        https://lmstudio.ai/_next/static/media/hero-still.3dc1eab6.png
			</image>
		</screenshot>
	</screenshots>

  <branding>
    <color type="primary" scheme_preference="light">#de8787ff</color>
    <color type="primary" scheme_preference="dark">#782121ff</color>
  </branding>

  <content_rating type="oars-1.1" />

  <url type="bugtracker">https://lmstudio.ai/</url>
  <url type="homepage">https://lmstudio.ai</url>
  <url type="contact">https://github.com/lmstudio-ai</url>
  <url type="contribute">https://github.com/lmstudio-ai</url>
  <url type="vcs-browser">https://github.com/lmstudio-ai</url>

  <releases>
    <release version="0.3.9" date="2025-01-30">
      <description>
        <p>
          0.3.9 - Release Notes
          Build 5

          Fixed an API bug where reasoning_content setting was not respected when streaming DeepSeek R1 chat completion responses
          Build 4

          New Experimental API: send reasoning_content in a separate field in chat completion responses (both streaming and non-streaming)
          Works for models that generate content within think think tags (like DeepSeek R1)
          Turn on in App Settings / Developer
          Build 3

          New: Add a Chat Appearance option to auto-expand newly added Thinking UI blocks
          New: Show quick access to guardrail config when the app gives an insufficient system resources error notification
          Fixed a bug where if the non-default models directory is deleted, new models will not be indexed
          Fixed a bug in hardware detection that sometimes incorrectly filtered out GPUs in multi-GPU setups when using the Vulkan backend
          Fixed a bug in the model load UI where F32 cache types without flash attention were not recognized as a valid configuration for the llama.cpp Metal runtime
          Build 2

          New: Added support for downloading models from nested folders in Hugging Face repositories
          Improved support for searching with Hugging Face URLs directly
          New: Automatically update selected Runtime Extension Packs (you can turn this off in Settings)
          New: Added an option to use LM Studio's Hugging Face proxy. This can help users who have trouble accessing Hugging Face directly
          New: KV Cache Quantization for MLX models (requires mlx-engine/0.3.0)
          My Models tab refresh: neater model names, and sidebar categories for model types
          Can toggle back to showing full file names in App Settings / General
          To see raw model metadata (previously: (i) button), right-click on the model name and choose "View Raw Metadata"
          Fixed a bug where clearing Top K in Sampling Settings would trigger an error
          Build 1

          New: TTL - optionally auto-unload unused API models after a certain amount of time (ttl field in request payload)
          For command line use: lms load --ttl seconds
          API reference: 

          New: Auto-Evict - optionally auto-unload previously loaded API models before loading new ones (control in App Settings)
          Fixed a bug where equations inside model thinking blocks would sometimes generate empty space below the block
          Fixed cases where text in toast notifications was not scrollable
          Fixed a bug where unchecking and checking Structured Output JSON would make the schema value disappear
          Fixed a bug where auto-scroll while generating would sometimes not allow scrolling up
          [Developer] Moved logging options to the Developer Logs panel header (••• menu)
          Fixed Chat Appearance font size option not scaling text in Thoughts block
        </p>
      </description>
    </release>
  </releases>
</component>
